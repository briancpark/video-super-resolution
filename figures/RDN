digraph {
	graph [size="36.6,36.6"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140382721259648 [label="
 (1, 1, 1200, 1200)" fillcolor=darkolivegreen1]
	140377058442304 [label=ConvolutionBackward0]
	140377058442256 -> 140377058442304
	140377058442256 [label=PixelShuffleBackward0]
	140377058441920 -> 140377058442256
	140377058441920 [label=ConvolutionBackward0]
	140377058441824 -> 140377058441920
	140377058441824 [label=AddBackward0]
	140377058441632 -> 140377058441824
	140377058441632 [label=ConvolutionBackward0]
	140377058441344 -> 140377058441632
	140377058441344 [label=ConvolutionBackward0]
	140377058438944 -> 140377058441344
	140377058438944 [label=CatBackward0]
	140377058438560 -> 140377058438944
	140377058438560 [label=AddBackward0]
	140377058438608 -> 140377058438560
	140377058438608 [label=ConvolutionBackward0]
	140377058437936 -> 140377058438608
	140377058437936 [label=CatBackward0]
	140377058434384 -> 140377058437936
	140377058434384 [label=CatBackward0]
	140377058438080 -> 140377058434384
	140377058438080 [label=CatBackward0]
	140377058438752 -> 140377058438080
	140377058438752 [label=ConvolutionBackward0]
	140377058441680 -> 140377058438752
	140377058441680 [label=ConvolutionBackward0]
	140377058435584 -> 140377058441680
	140377058398016 [label="SFF1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	140377058398016 -> 140377058435584
	140377058435584 [label=AccumulateGrad]
	140377058433760 -> 140377058441680
	140382721250528 [label="SFF1.bias
 (64)" fillcolor=lightblue]
	140382721250528 -> 140377058433760
	140377058433760 [label=AccumulateGrad]
	140377058435488 -> 140377058438752
	140382721250688 [label="SFF2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140382721250688 -> 140377058435488
	140377058435488 [label=AccumulateGrad]
	140377058433616 -> 140377058438752
	140382721250608 [label="SFF2.bias
 (64)" fillcolor=lightblue]
	140382721250608 -> 140377058433616
	140377058433616 [label=AccumulateGrad]
	140377058432128 -> 140377058438080
	140377058432128 [label=ReluBackward0]
	140377058435680 -> 140377058432128
	140377058435680 [label=ConvolutionBackward0]
	140377058438752 -> 140377058435680
	140377058438176 -> 140377058435680
	140382721250848 [label="RDB1.layer.0.conv.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140382721250848 -> 140377058438176
	140377058438176 [label=AccumulateGrad]
	140377058435632 -> 140377058435680
	140382721250928 [label="RDB1.layer.0.conv.bias
 (64)" fillcolor=lightblue]
	140382721250928 -> 140377058435632
	140377058435632 [label=AccumulateGrad]
	140377058435344 -> 140377058434384
	140377058435344 [label=ReluBackward0]
	140377058430880 -> 140377058435344
	140377058430880 [label=ConvolutionBackward0]
	140377058438080 -> 140377058430880
	140377058435728 -> 140377058430880
	140382721250768 [label="RDB1.layer.1.conv.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	140382721250768 -> 140377058435728
	140377058435728 [label=AccumulateGrad]
	140377058438272 -> 140377058430880
	140382721251088 [label="RDB1.layer.1.conv.bias
 (64)" fillcolor=lightblue]
	140382721251088 -> 140377058438272
	140377058438272 [label=AccumulateGrad]
	140377058438320 -> 140377058437936
	140377058438320 [label=ReluBackward0]
	140377058437792 -> 140377058438320
	140377058437792 [label=ConvolutionBackward0]
	140377058434384 -> 140377058437792
	140377058430736 -> 140377058437792
	140382721249808 [label="RDB1.layer.2.conv.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	140382721249808 -> 140377058430736
	140377058430736 [label=AccumulateGrad]
	140377058435776 -> 140377058437792
	140382721249888 [label="RDB1.layer.2.conv.bias
 (64)" fillcolor=lightblue]
	140382721249888 -> 140377058435776
	140377058435776 [label=AccumulateGrad]
	140377058438416 -> 140377058438608
	140382721250048 [label="RDB1.conv1x1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140382721250048 -> 140377058438416
	140377058438416 [label=AccumulateGrad]
	140377058438656 -> 140377058438608
	140382721250128 [label="RDB1.conv1x1.bias
 (64)" fillcolor=lightblue]
	140382721250128 -> 140377058438656
	140377058438656 [label=AccumulateGrad]
	140377058438752 -> 140377058438560
	140377058438800 -> 140377058438944
	140377058438800 [label=AddBackward0]
	140377058430784 -> 140377058438800
	140377058430784 [label=ConvolutionBackward0]
	140377058437840 -> 140377058430784
	140377058437840 [label=CatBackward0]
	140377058431408 -> 140377058437840
	140377058431408 [label=CatBackward0]
	140377058435248 -> 140377058431408
	140377058435248 [label=CatBackward0]
	140377058438560 -> 140377058435248
	140377058435392 -> 140377058435248
	140377058435392 [label=ReluBackward0]
	140377058430496 -> 140377058435392
	140377058430496 [label=ConvolutionBackward0]
	140377058438560 -> 140377058430496
	140377058442016 -> 140377058430496
	140382721250288 [label="RDB2.layer.0.conv.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140382721250288 -> 140377058442016
	140377058442016 [label=AccumulateGrad]
	140377058438368 -> 140377058430496
	140382721251008 [label="RDB2.layer.0.conv.bias
 (64)" fillcolor=lightblue]
	140382721251008 -> 140377058438368
	140377058438368 [label=AccumulateGrad]
	140377058437984 -> 140377058431408
	140377058437984 [label=ReluBackward0]
	140377058435296 -> 140377058437984
	140377058435296 [label=ConvolutionBackward0]
	140377058435248 -> 140377058435296
	140377058441536 -> 140377058435296
	140382721251248 [label="RDB2.layer.1.conv.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	140382721251248 -> 140377058441536
	140377058441536 [label=AccumulateGrad]
	140377058441440 -> 140377058435296
	140382721251328 [label="RDB2.layer.1.conv.bias
 (64)" fillcolor=lightblue]
	140382721251328 -> 140377058441440
	140377058441440 [label=AccumulateGrad]
	140377058432224 -> 140377058437840
	140377058432224 [label=ReluBackward0]
	140377058437744 -> 140377058432224
	140377058437744 [label=ConvolutionBackward0]
	140377058431408 -> 140377058437744
	140377058441968 -> 140377058437744
	140382721251488 [label="RDB2.layer.2.conv.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	140382721251488 -> 140377058441968
	140377058441968 [label=AccumulateGrad]
	140377058442208 -> 140377058437744
	140382721251568 [label="RDB2.layer.2.conv.bias
 (64)" fillcolor=lightblue]
	140382721251568 -> 140377058442208
	140377058442208 [label=AccumulateGrad]
	140377058427856 -> 140377058430784
	140382721251728 [label="RDB2.conv1x1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140382721251728 -> 140377058427856
	140377058427856 [label=AccumulateGrad]
	140377058438704 -> 140377058430784
	140382721251808 [label="RDB2.conv1x1.bias
 (64)" fillcolor=lightblue]
	140382721251808 -> 140377058438704
	140377058438704 [label=AccumulateGrad]
	140377058438560 -> 140377058438800
	140377058438848 -> 140377058438944
	140377058438848 [label=AddBackward0]
	140377058429152 -> 140377058438848
	140377058429152 [label=ConvolutionBackward0]
	140377058431600 -> 140377058429152
	140377058431600 [label=CatBackward0]
	140377058442784 -> 140377058431600
	140377058442784 [label=CatBackward0]
	140377058442496 -> 140377058442784
	140377058442496 [label=CatBackward0]
	140377058438800 -> 140377058442496
	140377058441296 -> 140377058442496
	140377058441296 [label=ReluBackward0]
	140382721302224 -> 140377058441296
	140382721302224 [label=ConvolutionBackward0]
	140377058438800 -> 140382721302224
	140382721301984 -> 140382721302224
	140382721251968 [label="RDB3.layer.0.conv.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140382721251968 -> 140382721301984
	140382721301984 [label=AccumulateGrad]
	140382721302032 -> 140382721302224
	140382721252048 [label="RDB3.layer.0.conv.bias
 (64)" fillcolor=lightblue]
	140382721252048 -> 140382721302032
	140382721302032 [label=AccumulateGrad]
	140377058442400 -> 140377058442784
	140377058442400 [label=ReluBackward0]
	140382721302416 -> 140377058442400
	140382721302416 [label=ConvolutionBackward0]
	140377058442496 -> 140382721302416
	140382721301792 -> 140382721302416
	140382721252208 [label="RDB3.layer.1.conv.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	140382721252208 -> 140382721301792
	140382721301792 [label=AccumulateGrad]
	140382721301888 -> 140382721302416
	140382721252288 [label="RDB3.layer.1.conv.bias
 (64)" fillcolor=lightblue]
	140382721252288 -> 140382721301888
	140382721301888 [label=AccumulateGrad]
	140377058443024 -> 140377058431600
	140377058443024 [label=ReluBackward0]
	140377058442688 -> 140377058443024
	140377058442688 [label=ConvolutionBackward0]
	140377058442784 -> 140377058442688
	140382721301648 -> 140377058442688
	140382721252448 [label="RDB3.layer.2.conv.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	140382721252448 -> 140382721301648
	140382721301648 [label=AccumulateGrad]
	140382721301696 -> 140377058442688
	140382721252528 [label="RDB3.layer.2.conv.bias
 (64)" fillcolor=lightblue]
	140382721252528 -> 140382721301696
	140382721301696 [label=AccumulateGrad]
	140377058438224 -> 140377058429152
	140382721252688 [label="RDB3.conv1x1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140382721252688 -> 140377058438224
	140377058438224 [label=AccumulateGrad]
	140377058434288 -> 140377058429152
	140382721252768 [label="RDB3.conv1x1.bias
 (64)" fillcolor=lightblue]
	140382721252768 -> 140377058434288
	140377058434288 [label=AccumulateGrad]
	140377058438800 -> 140377058438848
	140377058440720 -> 140377058441344
	140382721252928 [label="GFF1.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	140382721252928 -> 140377058440720
	140377058440720 [label=AccumulateGrad]
	140377058441056 -> 140377058441344
	140382721253008 [label="GFF1.bias
 (64)" fillcolor=lightblue]
	140382721253008 -> 140377058441056
	140377058441056 [label=AccumulateGrad]
	140377058441488 -> 140377058441632
	140382721249408 [label="GFF2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140382721249408 -> 140377058441488
	140377058441488 [label=AccumulateGrad]
	140377058441584 -> 140377058441632
	140382721249168 [label="GFF2.bias
 (64)" fillcolor=lightblue]
	140382721249168 -> 140377058441584
	140377058441584 [label=AccumulateGrad]
	140377058441680 -> 140377058441824
	140377058441872 -> 140377058441920
	140382721248928 [label="upconv.weight
 (1024, 64, 3, 3)" fillcolor=lightblue]
	140382721248928 -> 140377058441872
	140377058441872 [label=AccumulateGrad]
	140377058442112 -> 140377058441920
	140382721249088 [label="upconv.bias
 (1024)" fillcolor=lightblue]
	140382721249088 -> 140377058442112
	140377058442112 [label=AccumulateGrad]
	140377058442448 -> 140377058442304
	140382721248688 [label="conv2.weight
 (1, 64, 3, 3)" fillcolor=lightblue]
	140382721248688 -> 140377058442448
	140377058442448 [label=AccumulateGrad]
	140377058442544 -> 140377058442304
	140382721248608 [label="conv2.bias
 (1)" fillcolor=lightblue]
	140382721248608 -> 140377058442544
	140377058442544 [label=AccumulateGrad]
	140377058442304 -> 140382721259648
}
